{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 1.为什么Bagging降方差，boosting降偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏差是指预测值与真实值之间的差距，方差是指算法在相同样本数量的不用训练集上的方差。boosting是通过不断的迭代对y值进行逼近，每一轮的迭代都会针对损失函数降低误差，所以偏差越来越小；  \n",
    "而bagging是通过不断重采样，生成多个训练集，在每个训练集中训练一个学习器，最后整合的过程。比如在数量为m的初始训练集上，每次采样都放回，采样m次后得到一个采样集，训练一个学习器。bagging并不会显著的降低bias，因为在不同的训练集上用的算法其实是一样的。它其实只是增强了模型的鲁棒性，通过在不同的训练集上进行训练，数据扰动带来的误差会被降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A >2.随机森林与GBDT区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是bagging的一个扩展变体。RF在以决策树为基学习器构建bagging集成的基础上，引入了随机属性的选择。传统的决策树在选择划分属性的时候是在当前节点的属性集合中选择一个最优属性；而在RF中，对基学习器的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，再从子集中选择一个最优属性进行划分。如果k=d，那么基学习器和传统决策树相同；如果k=1，那么就是随机选择属性进行划分。RF的基学习器的多样性不仅来自样本扰动，而且来自属性扰动，最终泛化性能可以通过个体学习器的差异增加得到进一步提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT是一个boosting的算法，每一轮迭代都会生成一个决策树模型，最后的预测结果是把所有的弱学习器的结果相加。如果预测值看成自变量，那么boosting的过程就是损失函数在做梯度下降的过程，每一轮迭代但是在拟合伪残差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 3. 为什么xgboost效果不如随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost是boosting的过程，迭代过程主要是降低偏差，让预测更接近真实值。不如RF的可能原因有很多，如果xgboost训练的轮数不够，或者是其他一些超参数的调节不优，就会影响最后的结果。如果在调整的比较好的情况下，又是用的同样一组训练集，那么更可能的原因是xgboost产生了overfitting，产生了比较大的方差，导致在训练集的误差很小，但是在测试集上的误差比较大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于RF而言，由于它是基于bagging的集成算法，它会在使用重采样的方式生成多个训练集，然后训练多个学习器，最后取平均作为结果。另外，RF在选择分裂的属性的时候也加入了随机性，这样的方式更robust，所以能够比较好的降低方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上所述，xgboost不如随机森林的原因可能是训练集和测试集的分布出现了不一样的特点，导致boosting方法出现了泛化误差比较大的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 4. xgb对gbdt有什么提升，lgb对xgb有什么提升，为什么有了xgb还要使用lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost是高阶版的GBDT，它们之间主要有着以下不同：  \n",
    "原理上：  \n",
    "1. 传统GBDT主要采用了cart作为基学习器，而xgboost还可以支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的logistic回归（分类问题）或者线性回归（回归问题）。\n",
    "2. GBDT采用了一阶展开，而xgboost采用了泰勒二阶展开，二阶信息能让梯度收敛更快。 \n",
    "3. xgboost加入了正则项，用于控制模型的复杂度，降低过拟合风险。正则项惩罚了叶子节点数量和叶子节点的输出值的平方和。\n",
    "4. xgboost在计算分裂节点属性的时候，可以使用分桶算法，加快计算速度。同时也支持针对二阶导数的加权分桶。\n",
    "5. xgboost加入了缺失值处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "系统上：\n",
    "1. 并行的优化。不同维度特征排序可以同时进行，不同叶子节点的分裂可以同时进行。\n",
    "2. cache的hit miss优化\n",
    "3. 使用硬盘处理超大数据集。数据压缩之后再放到内存，用计算换IO时间；数据sharding到多个磁盘，读取时同时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM对于xgboost的提升主要体现在对于大数据集的处理上。xgboost在面对大数据集的时候，会出现计算速度慢的缺点。因为在选择分裂属性的时候，如果是精确求解，需要扫描每一个属性的每一个取值；即便是使用的近似解法，特征数量很多的时候需要扫描每一个特征。  \n",
    "所以，LightGBM的在样本维度和特征维度都做了优化。GOSS的采样可以大幅减少样本数，尽量保持梯度绝对值大的样本，同时保持数据分布不变。EFB把互斥特征绑定，减少需要扫描的特征数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 5. GBDT是如何做分裂的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 6. xgboost为什么要二阶展开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 7. 牛顿法与SGD区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 8. 牛顿法能用于非凸函数吗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 9. ID3, C4.5, CART是什么，有什么优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font face=\"微软雅黑\" color=#A52A2A > 10. 决策树是如何剪枝的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
